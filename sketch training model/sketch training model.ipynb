{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d9a42a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane.csv', 'alarm clock.csv', 'ambulance.csv', 'angel.csv', 'animal migration.csv']\n",
      "size of train : 106250\n",
      "Iteration 1000 -> Train Loss: 4.7897, MAP@3: 0.054\n",
      "Iteration 2000 -> Train Loss: 3.1041, MAP@3: 0.167\n",
      "Iteration 3000 -> Train Loss: 2.4988, MAP@3: 0.210\n",
      "Iteration 4000 -> Train Loss: 2.1680, MAP@3: 0.232\n",
      "Iteration 5000 -> Train Loss: 2.0067, MAP@3: 0.244\n",
      "Iteration 6000 -> Train Loss: 1.7360, MAP@3: 0.260\n",
      "Iteration 7000 -> Train Loss: 1.6343, MAP@3: 0.265\n",
      "Iteration 8000 -> Train Loss: 1.6091, MAP@3: 0.267\n",
      "Iteration 9000 -> Train Loss: 1.5509, MAP@3: 0.270\n",
      "Iteration 10000 -> Train Loss: 1.5151, MAP@3: 0.272\n",
      "Iteration 11000 -> Train Loss: 1.4597, MAP@3: 0.274\n",
      "Iteration 12000 -> Train Loss: 1.4604, MAP@3: 0.275\n",
      "Iteration 13000 -> Train Loss: 1.3650, MAP@3: 0.280\n",
      "Iteration 14000 -> Train Loss: 1.3453, MAP@3: 0.280\n",
      "Iteration 15000 -> Train Loss: 1.3591, MAP@3: 0.281\n",
      "Iteration 16000 -> Train Loss: 1.3144, MAP@3: 0.283\n",
      "Iteration 17000 -> Train Loss: 1.3350, MAP@3: 0.282\n",
      "Iteration 18000 -> Train Loss: 1.2995, MAP@3: 0.284\n",
      "Iteration 19000 -> Train Loss: 1.2654, MAP@3: 0.285\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "import os\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "en_dict = {}\n",
    "path = 'C:/Users/User/Desktop/photo search by sketch_final/train_simplified/'\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "print(filenames[:5])\n",
    "\n",
    "def encode_labels():\n",
    "    counter = 0\n",
    "    for fn in filenames:\n",
    "        en_dict[fn[:-4].split('/')[-1].replace(' ', '_')] = counter\n",
    "        counter += 1\n",
    "\n",
    "encode_labels()\n",
    "\n",
    "dec_dict = {v: k for k , v in en_dict.items()}\n",
    "\n",
    "def decode_labels(label):\n",
    "    return dec_dict[label]\n",
    "\n",
    "def get_label(nfile):\n",
    "    #print(nfile[:-4].split('/')[-1].replace(' ', '_'))\n",
    "    return en_dict[nfile[:-4].split('/')[-1].replace(' ', '_')]\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DoodleDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, mode='train', nrows=1000, skiprows=None, size=256, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        file = os.path.join(root_dir, csv_file)\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.doodle = pd.read_csv(file, usecols=['drawing'], nrows=nrows, skiprows=skiprows)\n",
    "        self.transform = transform\n",
    "        if self.mode == 'train':\n",
    "            self.label = get_label(csv_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _draw(raw_strokes, size=256, lw=6, time_color=True):\n",
    "        BASE_SIZE = 256\n",
    "        img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "\n",
    "        for t, stroke in enumerate(raw_strokes):\n",
    "            for i in range(len(stroke[0]) - 1):\n",
    "                color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "                _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n",
    "\n",
    "        if size != BASE_SIZE:\n",
    "            return cv2.resize(img, (size, size))\n",
    "\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doodle)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_strokes = ast.literal_eval(self.doodle.drawing[index])\n",
    "        sample = self._draw(raw_strokes, size=self.size, lw=2, time_color=True)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            return (sample[None] / 255).astype('float32'), self.label\n",
    "        else:\n",
    "            return (sample[None] / 255).astype('float32')\n",
    "\n",
    "SIZE = 224\n",
    "select_nrows= 10000\n",
    "\n",
    "doodles = ConcatDataset([DoodleDataset(fn.split('/')[-1], path, mode='train', nrows=select_nrows, skiprows=None, size=SIZE, transform=None) for fn in filenames])\n",
    "\n",
    "train_dataloader = DataLoader(doodles, batch_size=32, shuffle=True, num_workers=0)\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "for images, label in train_dataloader:\n",
    "    break\n",
    "\n",
    "\n",
    "def validation(get_loader, lossfn, scorefn):\n",
    "    model.eval()\n",
    "    loss, score = 0, 0\n",
    "    vlen = len(get_loader)\n",
    "\n",
    "    for X, y in get_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X)\n",
    "\n",
    "        loss += lossfn(output, y).item()\n",
    "        score += scorefn(output, y)[0].item()\n",
    "\n",
    "    model.train()\n",
    "    return loss / vlen, score / vlen\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(3,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def mapk(output, target, k=3):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output (torch.Tensor): A Tensor of predicted elements.\n",
    "                           Shape: (N,C)  where C = number of classes, N = batch size\n",
    "    target (torch.int): A Tensor of elements that are to be predicted.\n",
    "                        Shape: (N) where each value is  0≤targets[i]≤C−1\n",
    "    k (int, optional): The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score (torch.float):  The mean average precision at k over the output\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(k, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "        for i in range(k):\n",
    "            correct[i] = correct[i] * (k - i)\n",
    "\n",
    "        score = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        score.mul_(1.0 / (k * batch_size))\n",
    "        return score\n",
    "\n",
    "a = torch.randn(10,2,220,200)\n",
    "\n",
    "k=5\n",
    "a[:k].view(-1)\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "def squeeze_weights(m):\n",
    "    m.weight.data = m.weight.data.sum(dim=1)[:,None]\n",
    "    m.in_channels = 1\n",
    "\n",
    "model.conv1.apply(squeeze_weights)\n",
    "\n",
    "num_classes = 340\n",
    "\n",
    "model.fc = nn.Linear(512, out_features=num_classes, bias=True)\n",
    "\n",
    "model(torch.randn(12,1,224,224)).size()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, amsgrad = True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5000, 12000, 18000], gamma=0.5)\n",
    "\n",
    "epochs = 1\n",
    "lsize = len(train_dataloader)\n",
    "print(f\"size of train : {lsize}\")\n",
    "itr = 1\n",
    "p_itr = 1000\n",
    "model.train()\n",
    "tloss, score = 0, 0\n",
    "for epoch in range(epochs):\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        tloss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        score += mapk(output, y)[0].item()\n",
    "        scheduler.step()\n",
    "\n",
    "        if itr % p_itr == 0:\n",
    "            print('Iteration {} -> Train Loss: {:.4f}, MAP@3: {:.3f}'.format(itr, tloss / p_itr, score / p_itr))\n",
    "            tloss, score = 0, 0\n",
    "        itr += 1\n",
    "        if itr >= 20000:\n",
    "            break\n",
    "            # you can continue this for better model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5f012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 89.86it/s]\n"
     ]
    }
   ],
   "source": [
    "testset = DoodleDataset('test_simplified.csv', 'C:/Users/User/Desktop/photo search by sketch_final/', mode='test', nrows=None, size=SIZE)\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "labels = np.empty((0,1))\n",
    "#labels = labels.to(device)\n",
    "for x in tqdm.tqdm(testloader):\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    labels = np.concatenate([labels, pred.cpu()], axis = 0)\n",
    "\n",
    "submission = pd.read_csv('C:/Users/User/Desktop/photo search by sketch_final/test_simplified.csv', index_col='key_id')\n",
    "submission.drop(['countrycode', 'drawing'], axis=1, inplace=True)\n",
    "submission['word'] = ''\n",
    "for i, label in enumerate(labels):\n",
    "    submission.word.iloc[i] = \" \".join([decode_labels(l) for l in label])\n",
    "\n",
    "submission.to_csv('C:/Users/User/Desktop/photo search by sketch_final/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
